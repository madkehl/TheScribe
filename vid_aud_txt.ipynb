{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/extracting-speech-from-video-using-python-f0ec7e312d38\n",
    "#https://stackoverflow.com/questions/37999150/how-to-split-a-wav-file-into-multiple-wav-files\n",
    "import speech_recognition as sr \n",
    "import moviepy.editor as mp\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitWavAudioMubin():\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.filename = filename\n",
    "        self.filepath = folder + '/' + filename\n",
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def get_duration(self):\n",
    "        return self.audio.duration_seconds\n",
    "    \n",
    "    def single_split(self, from_min, to_min, split_filename):\n",
    "        t1 = from_min * 60 * 1000\n",
    "        t2 = to_min * 60 * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
    "        split_audio.export(self.folder + '/' + split_filename, format=\"wav\")\n",
    "        \n",
    "    def multiple_split(self, min_per_split):\n",
    "        total_mins = math.ceil(self.get_duration() / 60)\n",
    "        for i in range(0, total_mins, min_per_split):\n",
    "            split_fn = str(i) + '_' + self.filename\n",
    "            self.single_split(i, i+min_per_split, split_fn)\n",
    "            print(str(i) + ' Done')\n",
    "            if i == total_mins - min_per_split:\n",
    "                print('All splited successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_path = r'2351285129819900164.mp4'\n",
    "#movie_path = r'5433273397410606852.mp4'\n",
    "movie_path = r'2366109844211546884.mp4'\n",
    "clip = mp.VideoFileClip(movie_path) \n",
    "current_vid = 'vid3.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 355/84811 [00:00<00:23, 3544.05it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./trial/trial.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "os.mkdir('./trial')\n",
    "clip.audio.write_audiofile(r'./trial/trial.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Done\n",
      "1 Done\n",
      "2 Done\n",
      "3 Done\n",
      "4 Done\n",
      "5 Done\n",
      "6 Done\n",
      "7 Done\n",
      "8 Done\n",
      "9 Done\n",
      "10 Done\n",
      "11 Done\n",
      "12 Done\n",
      "13 Done\n",
      "14 Done\n",
      "15 Done\n",
      "16 Done\n",
      "17 Done\n",
      "18 Done\n",
      "19 Done\n",
      "20 Done\n",
      "21 Done\n",
      "22 Done\n",
      "23 Done\n",
      "24 Done\n",
      "25 Done\n",
      "26 Done\n",
      "27 Done\n",
      "28 Done\n",
      "29 Done\n",
      "30 Done\n",
      "31 Done\n",
      "32 Done\n",
      "33 Done\n",
      "34 Done\n",
      "35 Done\n",
      "36 Done\n",
      "37 Done\n",
      "38 Done\n",
      "39 Done\n",
      "40 Done\n",
      "41 Done\n",
      "42 Done\n",
      "43 Done\n",
      "44 Done\n",
      "45 Done\n",
      "46 Done\n",
      "47 Done\n",
      "48 Done\n",
      "49 Done\n",
      "50 Done\n",
      "51 Done\n",
      "52 Done\n",
      "53 Done\n",
      "54 Done\n",
      "55 Done\n",
      "56 Done\n",
      "57 Done\n",
      "58 Done\n",
      "59 Done\n",
      "60 Done\n",
      "61 Done\n",
      "62 Done\n",
      "63 Done\n",
      "64 Done\n",
      "All splited successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = './trial'\n",
    "file = 'trial.wav'\n",
    "split_wav = SplitWavAudioMubin(folder, file)\n",
    "split_wav.multiple_split(min_per_split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcript': \"hello everyone my name is Tracy King and I would like to welcome you to today's Andy a data harmonization webinar a portion of today's webinar will be played using your computer audio this means that if you dial into the audio conference using your phone audio you will hear the video play through your computer or devices speakers rather than over the telephone if you are not hearing the audio properly whether you're connected via the computer or the phone you should check the volume on your computer speakers in or check the device output it's possible that the audio is playing through a different output output like a USB headset if for any reason during today's webinar we experience any technical difficulties we will\", 'confidence': 0.73088163}\n",
      "{'transcript': 'fried you with a recorded webinar link via email Additionally you can request previously recorded webinars at Anytime by emailing the NDA help desk at NDA help at mail. Nih.gov that information will be provided at the end of the webinar as well just a note that we are not going to answer all the questions at the end of the webinar so please email us any questions that you have and we will get back to you as soon as possible with that we will start the webinar', 'confidence': 0.63423496}\n",
      "{'transcript': \"today we'll be going over now to take the data that you are collecting as part of your NIH Grant and harmonize it with the NDA data standard by harmonize we mean take steps to make sure your data are consistent as possible with data from other Laboratories when it is deposited into the NBA system so in our presentation today I'll be going through a slide deck quickly to give an overview of the different pieces of the data harmonization procedure and then I will be going to the actual NDA website in order to give you a live demonstration of some of these features will be discussing and tools and had to approach this so with that let's get started if you have any questions during the course of this presentation please go ahead and type those into the go to\", 'confidence': 0.60767585}\n",
      "{'transcript': \"client the question section there and we will go over all of the questions at the end so first let's discuss our webinar program in general so this is as I said the India data harmonization webinar this is the second in a series of for webinars we give the first being an overall orientation for new grantees that covers the basics of the entire data sharing process this presentation as I said his own and you did a harmonization this is kind of the step of taking the data you are collecting and preparing it for submission into the infrastructure for sharing we also have a webinar on how to submit the data itself entire webinar just on that and then we also have a fourth webinar on accessing data in the system now all data in NDA are eventually shared for reuse with the research community that 4th webinar is on how to do\", 'confidence': 0.71338898}\n",
      "{'transcript': \"the set of all the measures instruments data types that are defined in NDA as having a standard structure and your data will need to match that or have a new definition to find to accommodate it so will cover the data dictionary will cover how to set up your data expected list that is a deliverable expected as part of the data sharing process within 6 months of your gramps award and as you as you completed the day is commission agreement which will have initiated your project in giving you control over your collection in NDA it expected list will be the next step in that process that's our primary tracking mechanism for your project going to be covering that today in detail we're also going to understand the data harmonization process so how you're going to approach this process and it is an ongoing process of harmonizing your data to allow it to be submitted\", 'confidence': 0.62001228}\n",
      "{'transcript': \"share to India maximizing its value for reuse and is always most importantly we will understand how to get help how to find more information how to get the support that you need in order to complete these other steps successfully so given that we will move on to this this / overview of the days during process this presentation will cover kind of the middle steps this kind of linear chart shows you know the overall process from when your you know initially applying for your Grant all the way up until close out and what we're going to be covering now is kind of this second row of getting your good words created to find data expected uploading your data will we actually want to discuss uploading or date ever will you know we're going up to that point we working with the data dictionary to get it harmonize so this presentation is not going to cover the sort of startup points those are in the orientation\", 'confidence': 0.70003515}\n",
      "{'transcript': \"we're also not going to be covering how to prepare your manuscripts or publish or actually upload the data or the QA process that's all and I recovered in the next women are the submission one so just as a sort of illustration of where we are proud of you oriented in this process so with that let's go ahead and dive in on with the first real sort of subject matter here as I mentioned the gooey this is an abbreviation for the global unique identifier it's the good itself is a series of alphanumeric characters its letters and numbers and it's generated by a tool the tool is a piece of software that you download onto your local computer and then run login and use it to create these identifiers you create these IDs by entering participant information pii from their birth certificate into the tools interface so\", 'confidence': 0.68403524}\n",
      "{'transcript': \"denture in the least of these data that tool locally on your computer is going to use those to create a series of one-way hash codes those hash codes which are not your from which these data are not deducible is send that's what sent an IH and then that is matched against a database of other kinds of hash code like that if a matches found that existing fluid is returned otherwise a new one is greater than that has returned to you and can be used for that person in this way the data from the same individual in different potential studies can be provided it can be linked across time and space using this ID without anyone ever having access to or sharing any of their personal information to increase the value of all those data it is going to be present in all NBA data as you go\", 'confidence': 0.66638261}\n",
      "{'transcript': \"mentally a list of these structures which represent are recognized in to find data on surf standard shells each of these structures is itself to list of these elements of each of these elements is a variable or question or an item on one of these instruments the dictionary as a web page and cool also allows you to generate blank template files for submitting it allows you to check all the you know definitions for all the structures it's a powerful resource throughout the entire day shipping process at the bottom of the slide will see you screenshot of the new dictionary search page in real life a little bit later expected list within 6 months of your award and it is something in your projects collection that you would need to go in and Define or have someone to move delegated this privilege to go in and Define it on the website it is our\", 'confidence': 0.64740229}\n",
      "{'transcript': \"harmonization is constructed so that you know what you going crazy this list we're going to be working with you and that's how you know this process is initiated in and worked out so at the moment we have these two different data sharing the schedules that can be used to determine what dates based on your own projects individual start and end dates there is the first raw data also known as descriptive data and some of our documents and these are they that you're collecting and over the course of your project as it progresses you were depositing them into the system every 6 months during our standard biannual cumulative upload. And then it's your 4 months later after it's been qaid this is.\", 'confidence': 0.69392771}\n",
      "{'transcript': \"contrasted with analyzed data which are data that are related to your primary aims data that you are generating novelly that will be inappropriate for sharing with a general Community until it is published and that is shared when it's published right away or all the date in your project you should one year after your project end date including all of these data what should be submitted in as soon as possible now I will know that this is changing as of January of 2019 these distinctions will disappear and all data will be shared one year after project end date including the first no-cost extension that is for projects starting effective January of 2020 and you can contact NBA help that mailed NH. Gov for any clarifications about the schedule which is in\", 'confidence': 0.9746604}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcript': \"transition now for some data types this is just to illustrate sort of what I'm talking about when I discuss these different types and it also characterized each of the types and in terms of what needs to be done in addition to the actual date itself what sort of other tasks were required before it can be uploaded based on some of these more complex types complex in the sense of in which they are uploaded to our system so first we can consider dating Michael clinical assessments essentially these are just spreadsheets ultimately when these data are prepared and they are submitted to NDA they will be most likely in the form of a CSV spreadsheet now there are alternative ways of doing it but the basic sort of default process is that it's ESB Spreadsheet will be uploaded these clinical assessment data types are the data that\", 'confidence': 0.71415734}\n",
      "{'transcript': \"consists of the spreadsheet they don't have a source files they don't have a source e ated additional pages in the website that need to be updated to provide the right metadata and linkages to the metadata so those are spreadsheets then we Imaging and non-functional imaging these are a CSP spreadsheet all the data will have an Associated CSP spreadsheet and this is the standard image of three structure and then it will have these Associated files the raw data files themselves maybe a dicom file for Imaging likewise fmri will have an Associated spreadsheet you will have Associated files with this image 03 spreadsheet which is a dictionary structure for Imaging data and then you also have to define the experiment experiment definition is something in NDA on the website that you will need to create and then linked with your data in that spreadsheet in order to upload it\", 'confidence': 0.97478825}\n",
      "{'transcript': \"that definition is also required for these other data types that all involve a kind of experiment that needs to be run and the purpose of these definitions is to allow someone to reproduce it as easily as possible and that includes this EG Amex data genomix eight other kinds of Amex data as well as I tracking data then when you see image 03 hear Yeezy Supply one these these terms of each of those and that's where the format is when you make them to recognize as the idea of a data structure and these are those very data structures I was referring to earlier the sort of type specific data structures so the structure EG sub files are one is a data structure generically for all raw EEG data regardless of the experiment you're running or any you know tasks that may be performed during the experiment those are defined in their own structures and the actual data on your scanners are the equipment those are what's in the news\", 'confidence': 0.97068596}\n",
      "{'transcript': \"3rd and R Series this is the second in our Series so we're not going to cover the stool today but the two old is what takes in those files and it validates them against the appropriate data dictionary structure to make sure that all of the data within it or matching or harmonize to the standard that you define for your own data within that structure so it's going to check that values are within ranges that are defined that the fields required for upload or not missing and so on and it will make sure that it has valid source files experiment definition when the checks if that's appropriate and then we'll packaging upload the data so essentially this is a post collection free upload keyway check on the data before it's even submitted we have other qhx that are performed after its submitted on those can be discussed in the third webinar free upload a check and you can use this\", 'confidence': 0.96830338}\n",
      "{'transcript': \"invalidation in submission workspace and then we also have our a data Dictionary tool on our end which is an internal tool that allows us to create project-specific Alias season translations for your collection will discuss with those mean exactly in detail later and as I said at the beginning it's all you can see is available as our email help desk and then 301-443-3265 is our phone number we are available during all normal business hours now we are on the East Coast so if time zones are a factor but these are also available we'll go back to that when I get to the city and we can show those contact information later but for now let's move on to the actual website demonstration phase now that are sort of over view slideshow is complete so now I'm going to show you what the actual dual degree good looks like with addiction\", 'confidence': 0.9585759}\n",
      "{'transcript': \"she looks like and how you actually use that webpage how you actually go in and create your data expected list and finally I will also be showing you a just an example of what what the data will actually look like in your spreadsheets so first however let's take a look at some web resources so this is the NBA website I'll back up ones you can see the homepage indicated nth. Gov is our website and under our section I'm contributing data you can see we have MD-88 of standards and NDA harmonization approach so we're going to be going over you know how to use the site in this presentation as as kind of a resource however if you're interested in documentation if you prefer the visual learning of reading text we do have these two pages\", 'confidence': 0.68183142}\n",
      "{'transcript': \"1 documents on what is expected for each different kind of data kinds of data we've seen before and how it's expected to be dealt with and then we also have a document outlining our general harmonization approach so this covers a lot of what I'll be going over today in terms of what kind of goes back and forth between members of your project and our data dictionary cute curators way with I like this section overall is a good resource for information on date of submission in general and I would highlight these two pages in particular has good resources for information specifically D-Day armatization piece of submission that said let's move over to the website and take a look at the good tool so I have my gujral hears I'm going to launch it and as you see when I click launch it's downloading this file as I mentioned before this is a file you download locally in\", 'confidence': 0.96758902}\n",
      "{'transcript': \"beer on your system so that we can avoid sending any of the pii to us this does require Java unfortunately that version of java 8 is required in order to run this application on your system so that needs to be installed I'm going to close my console here since you probably won't see that so I'm going to log in with my credentials so that I can use my normal NDA username and password to log into that you can create one of those from the homepage and then you just need to make sure you register for the good tool on your profile page after you created your account now once I accept the warning message I see that will really as it is quite a simple graphic interface for entering the data it's just a basic double-entry interface for each of the fields\", 'confidence': 0.73366338}\n",
      "{'transcript': \"you would enter in their first name last name we have this answer regarding their middle name if they have a middle name and must be entered here and then as I mentioned before their birthday their sex at Birth and their City Municipality of birth so once you have all these information entered you would basically just could generate go with I'm not going to enter in a lot of information now but you would like generate go ahead and the goo it would appear right here in this little box of this copy to clipboard button which copy and paste it into a document you know where you're maintaining it appropriately so that's basically all there is to creating a single gooey this new button clears the form and then exit closes the tool now if you're creating a time and then copying and pasting them into a document to maintain\", 'confidence': 0.78651059}\n",
      "{'transcript': \"all there is to know about this however the tool does have a few other features that I would highlight so first the tool can create multiple gueits so to do that I would click first done this good template link it's going to download this CSV Excel file I can as you see here it's populated with sample information different various different historical figures information is in surgery as a placeholder so you would delete all of that leaving this ID the way it is this is just a sequential ID that you might need to fill out if you know have many more people and you can enter in all the day to hear the only difference I would say is this use existing good flag this should be set to yes for everyone in your spreadsheet unless you know for a fact that they are a high-risk hair within this sheet for a false positive so\", 'confidence': 0.70522839}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcript': \"you can also use this tool to create Sudoku words one at a time so is sudoku good witch looks like this this is also very that you know what I normally would look like this but it won't have this_I envy in the middle of it is unlike a good it's just random it's arbitrary it's a totally random sequences of information that has no bearing on the person's VII it's not linked to anything so is sudoku what is appropriate in cases where for whatever reason it's totally impossible to get the data from the in order to create the good for those participants so for example if someone declined to give you their birthday or something like that you can still up there upload their data but you would you could use a Sudoku or if your project you know if it was like retrospective in you\", 'confidence': 0.79375333}\n",
      "{'transcript': \"contact and then cases like this the suit of wood can be used to use Sudoku aids for cases that are not just went off participants declining to provide all of this information while still consenting to have their data shared you need to email NDA help ml to IAH. Gov to check in about that and you also would need to check in with the help desk in order to get more than one soda go to Once because this will cannot do that so if you need many you will need to contact the helpdesk also be used as a placeholder to upload data for participants while you're you know in the process of getting these data so with that I'll highlight how to convert a pseudo good so when I click converts to toggle Dover to this interface you'll notice it's exactly the same\", 'confidence': 0.63968533}\n",
      "{'transcript': 'except it has this pseudogout field this is for a promoting a pseudo go into a full Google if data have been uploaded using the pseudo do it and at a later date the actual data required for a full give it is collected now you can also do this in bulk and the promotion template here and he convert multiple Shooters option here will perform that function in exactly the same way that now that will does it for normal Goods generate IDs for all of your participants so if you know but if we are and so the next step will be going into your collect', 'confidence': 0.72259718}\n",
      "{'transcript': \"hole is not really going to be an option regardless of what you use some of that I have this text search field I can type in there like an applied that with just a text and that's going to reduce a little bit but there's still you know there's a lot of anxiety has his regulars 44 so I can also filter on different let's say I'm only interested in well just NDA will narrow it down that way in a fly and I guess they're all classified as NDA because I still 44 results in addition to that you can search by category one or multiple selections datatype and then apply our reset filter using these buns so this allows you to narrow down the 3,000 data structures that are currently defined and order to find the one that you are looking for and so how would you determine which one to look for in the first place\", 'confidence': 0.64201015}\n",
      "{'transcript': \"when you go to create your data expected list it will more or less look like this it will have some dates that were set on this item first of all of one item on it most likely if you're collecting genomic data of some kind will probably have to but at a minimum will have this one will have a research subject and pedigree so this this structure research subject can pedigree which is least to directly right here and are subject to one is it's ID so here's that I hear the data structure page now every project in the every project that shares data through nda's expected to provide this this day to structure this is a summary structure that provides you know just one record per subject information that characterizes that subject and then you know when is this is relevant especially in genomic studies degree information things like that it's a summary structure that are query tools use\", 'confidence': 0.71655595}\n",
      "{'transcript': \"and therefore is is necessary for everyone to upload it's probably not something you're collecting specifically as part of your project and if we go to the page you can see you know everyone is doing it there's over three hundred thousand participants who have this structure in the system so this is included by default in your date expected list as sort of the seed item it's therefore likely to be the only one when you come in so when there's only one here and it says research of us can pedigree they will actually have a charge enrollment of one probably and then I will have an initial submission date and initial sharing day that are set based on the sort of default schedule for projects and your lord so the dates that you will see here are going to be probably the First Data submission. For your schedule that is more than six months after your award was was given\", 'confidence': 0.72081906}\n",
      "{'transcript': \"so if you were if your Grant was awarded in let's say let's say December of 2019 and your submission schedule operates on the April October biannual submission periods than your initial submission dates seated in your date expected list would be October of 2020 so in terms of the difference submission dates you should expect to see a July January submission Paradigm if you are getting a grant from the National Institute of Mental Health and you should expect to see in April October Paradigm if you were if you were awarded the grand from and I AAA the Nationals do velka Hall abuse and alcoholism so this is pretty much Mental Health Data expected\", 'confidence': 0.96031928}\n",
      "{'transcript': \"July 2019 so how do we actually break this list and how does it relate at all to do dictionary that I was just showing you well when I click activate expected I can see I have this dialogue and it prompts me to enter enrollment so let's say I'm going to continue this trend of having originally 100 I'll I'll duplicate the initial submission date that I've seen here and the initial sharing date on just so they're consistent and then I'll search so you can see I'm searching for I was going to search for research subjects of pedigree but that's already on here so let's search for ABC Community books you can see I typed it too fast and it overrode the actual title string so it ever end behavior checklist this structure this is one of our clinical instruments\", 'confidence': 0.97111791}\n",
      "{'transcript': \"I'm adding that this is an example so I went ahead and added that and you can see now it's on the list I'll go ahead and save just to be safe it should save automatically but it never hurts to save so I can head it this I can delete it once you upload data you won't be able to delete these items anymore but you can edit it in order to change the target enrollment number which is expected to be the final unique count of subjects in that structure the submission date in the sharing day or adjustable as your project progresses so I'll cancel that and then I'll add a new item so I'll add 100 let's give it the same dates just for consistency and let's say I'm also collecting raw Imaging data you can see I search for image and even picked up on it from the data structure search year this list\", 'confidence': 0.70378518}\n",
      "{'transcript': \"what's getting displayed here is just coming from the data dictionary these are the titles so images the title body image States scales the Title VII SSO one image of 3M riq mo1 these things are the superstructure IDS in a data dictionary so by selecting this I'm indicating hear that I'm collecting raw Imaging data that will be uploaded in July of 2019 and is getting added to the list so okay there we go and once again I'll save just to be safe although it should save automatically know any case you will be adding the data structures that you've identified in the data dictionary as matching or being usable for your project so one way to approach this would be to look at your Grant will be your project and determine\", 'confidence': 0.96522915}\n",
      "{'transcript': \"you know what you're collecting and go to the data dictionary and find a structure that corresponds to all of the instruments you're collecting all data types you're collecting and once all your data or accounted for you can go in and added to the data expected list know that that would be one way of approaching it is for the simplified fashion what I would recommend that you do instead and the way this will sort of play out the way we sort of expect this to play out is that you can sort of start with those you know those that you can find that a very straightforward that you can identify as being either usable outright or they need to be modified or you can start by identifying ones that don't exist at all obviously and then adding them as new items which I'll show you in just one second but the point I'm getting to is the most important part of this is just get started on adding items today to expected\", 'confidence': 0.73873591}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcript': \"and try to make sure everything's squared away in time for submission in the event that you do have structures that need to be Define from scratch so say you're collecting something and you check the data dictionary and you can't find any example of it or anything even remotely close to me entirely new structure that's also done for the day to expected list of the most common questions we get is how do I Define an entirely novel structure in your system for new daytime collecting well-to-do that you would like to admit it expected and you add it just like these others except that you would click upload definition and toggle to that menu instead of searching but it is structured title let's say I'm still collecting a hundred subjects data and we'll stick with our arbitrary dates just for consistency but again these dates you'll be setting based on your own projects expectations\", 'confidence': 0.66644931}\n",
      "{'transcript': \"I would provide a title so where to find this from scratch so we'll need a title and then you would pick a file to upload so let's say this file this PDF file and upload here can be a PDF file if it you if we are uploading is a is a question are any of the scanned copy of it including all of the dinosaurs manuals and coding in a PDF file that would be perfectly appropriate it could also be a zipped Archive of multiple files basically you just need to attach here whatever is necessary in your opinion for NDA to go in take a look at it and create an initial draft the structure so this is what we want to see you in terms of new structure definition we have a name we have a file and then add a turkey reader can take that and get started on drafting it out and once they're done with it\", 'confidence': 0.66634965}\n",
      "{'transcript': \"they'll contact you when you can kind of go back and forth with him to make sure everything works the way you want it to so we've discussed the native dictionary and how to find structures we've discussed how to add those to the date expected list we've discussed how to add new structure requests to the date expected list so before we move on to an actual file and then onto experiments let's take a look at a few structures themselves so that you're familiar with it and then that will help as we go into the actual files so I'm going to pull back up this example that I found of this community checklist so when you go into a structure it will look like this you will have this kind of header section and then the structure itself is just as list of elements so as you can see here\", 'confidence': 0.65026921}\n",
      "{'transcript': \"this initial block of elements includes his comments element but sex interview age interviewed a thesaurus subject ID which is your internal ID for the participant and the subject ID which is a good of the participant so those five elements are required in every single record of every single structure as the kind of cord identifiers for this record everything else in this structure is is an item or a question in this instrument so you can see we have the name there's a tie provided there's a size provided there's a status of whether it's required or recommended there's a description value range there's a notes column which typically contains the coding for the values you can see that down here here's an example of the notes column being used to provide the underlined\", 'confidence': 0.69999278}\n",
      "{'transcript': \"values that the code stand for and then there's an alias, so there are a few things I want to highlight here first is the required versus recommended status in this required column a column being recommended just means that if you don't have data in that field then the system will not block you from uploading so the rule for this is that if you collected the data and it's in a recommended column then it must be provided if it's in a required column then the system will technically block you from uploading unless you know if you don't have a value in that field now the other pieces of this year the size the value range the notes where the sort of coding is explained these are all the things that that are real\", 'confidence': 0.67143744}\n",
      "{'transcript': \"give the definition those are what your defining that's the what constitutes this structure and what the validation tool is going to be checking to try to catch any any mistakes or errors in the use of the standard or or the date it's been entered into these fields now. A couple occasions so far I've mentioned mentioned that I've mentioned translation rules these are two rules you know we have access to that we can use to make your job a lot easier so first I go to Alias he's so in this column you can see there's no Google Earth as an alias of subject key ABC, Minter whr home is an alias for inter whr home so element name so when you upload these data each of the columns in your spreadsheet actually as we discussed this this would be a good time to introduce you to\", 'confidence': 0.73125488}\n",
      "{'transcript': \"the first two rows to expand this column you can see we've ABC Community into and that matches ABC Community here and to ABC Community O2 these two columns in the first header have to be the two components of this so-called short name the idea of the structure that's how the tool and our system knows which structure this file is so that's necessary second is the row of elements you'll notice that this is basically just the element name column from the definition transposed that's exactly what it is so each of these is one of the elements and let's go ahead and open now one that has data in it and then you'll be able to see what I'm talking about a little bit better so here we go this is fake data and you can see this is basic\", 'confidence': 0.60110962}\n",
      "{'transcript': \"well it's fake data so this is what it looks like with data in it each of the rows is one subject at one time points record and then each column is an element and in each row for an individual's visit or time pulling it will have the data of the value for that visit or time point so you can see here how it's organized in the spreadsheet ultimately and this also demonstrates what your what your data will look like if they are longitudinal so if you have you know this is a spreadsheet is actually a demonstration of what it would look like if you had two subjects who were each coming in for five different visits each a month apart they have the same ID they have a different date and then their age is increasing the age has to be in months by the way that's why it's 100 and so on so the age and month is going\", 'confidence': 0.78200406}\n",
      "{'transcript': \"that's matching the time passage and then the IDS are the same and each column contains the appropriate data value for that visit and this is obviously a very simple example But ultimately this is what your data will look like it will be like this in spreadsheets with the subject ID in the data kind of going out in the columns so if you can configure away on your end to have you know those files results automatically from your own internal capture system that that's something that's people found very positive or or you know manual data entry is kind of a guest of the ultimate fallback position so in terms of the actual structure this is what we were talking about so is structural like this with elements and all the different parameters Define is going to be ultimately required for all of the data you or upload\", 'confidence': 0.84438235}\n",
      "{'transcript': \"SO2 going to go back to our framing here we have been working on the data expected list we've used the data dictionary to identify structures that need to be created from the scratch I've added my request for that to the list I got an existing structures we can use right out of the box to the list and one thing that doesn't happen if it is expected list is potentially edits to existing structures once you have a data curator those requests can be sent to that person directly via email email to NDA help that mailed NH. Gov our help desk address and now we know what our data will you know we'll need to start to look like so now just to wrap up I'm going to show you what it looks like with sort of a more complicated kind of data with imaging data\", 'confidence': 0.82057911}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcript': \"and then I will take a quick look experiments and then that will wrap up the live presentation so to do that I have my image 03 spreadsheet prepared and as I said before you know even these other types they're all going to have a CSP spreadsheet like this this one is just to be pointing to other files as well so here we have the same kind of spreadsheet the elements out here in these columns and then you can see this is the same person coming in once per month five times in a row so in in most respects this is exactly the same as the spreadsheet I was just showing you the date and just need to be populated in these columns the key differences are associate files and experiments which of these two columns so first let's deal with Associated files so this has a\", 'confidence': 0.96651292}\n",
      "{'transcript': \"call the image file similar structures and by similar I mean the other structures that are type specific file metadata structures like EEG subject and sell on d-sub files rather they also have a file type element like this and what goes in that element is not data you collected but it's actually passed as file in question so subject X here in their visit on January 1st 2009 at cetera had these cans in these files taken and then you know there's data about the rest of the files Android rest of this spreadsheet so you can zip up all the scans that were done by one person under the same parameters of the same day and upload element there are ultimately more there more file elements you can use to add more data up to up to summon number of individual files\", 'confidence': 0.97317094}\n",
      "{'transcript': \"prefer not to zip too many of them and then they need to be located in the directory on your local system the same system as this file and then the tool as it parses this spreadsheet will automatically go and identify those files and get them so you can see it here I have really the absolute path starting at the drive-thru and then going up through my user folder into this demonstration folder named intro 3 you'll notice this duplicate exactly the directory structure I see when I go hear my demonstration folder and then this image. PNG is of course exactly the file on referencing Peter and obviously you know if you're uploading a zip file full of diatoms for this person that you know it's not going to be this simple P&G but this this is the principle underlying the organization of this this must take a path to the file in question you can be an abscess\", 'confidence': 0.68939316}\n",
      "{'transcript': \"past like this they can also be a relative passed starting at the location the working directory of this csb spreadsheet itself as when it goes into the tool but you can't go wrong with an absolute path like this and once those are populated correctly the tool will check that if it fails to find one you'll you'll know and you'll need to be able to it you need to make sure he's at the path is correct for the you know the directory structure is correct so that's how the files are uploaded it's it's basically just as simple as getting the path in the file type element and then let's take a look now and experiments so require an experiment only functional MRI does and then the other experiment types do like EEG you know makes sense so on so for that reason experiment ID here is going to be empty for those and in this purported fmri record it will be filled in\", 'confidence': 0.63055056}\n",
      "{'transcript': \"you can see the scan types different that's how I know that's fmri so let's take a look at image of three you can act as a sort of navigation Aid hear these little eyes can show you links directly to the structures reach of your data expected items so here is image here's my image file structure and here's my experiment it so this is obviously well it's conditional so a few of these are conditional and those go into these logical operators and if the operators Matt then it becomes required and if not then it's it's not necessary so when the scan type is fmri instead of us can type is another element here so scan take to be any of these recognized Imaging format so this can type is fmri then an experiment ID must be provided so let's go back to our collection and take a look at the experiment stabbed which is\", 'confidence': 0.69886917}\n",
      "{'transcript': \"those are Define so first I would click add new experiment attic new experiment you can copy existing experiments as sort of a base to start from in events where the experiments very similar so let's stick with fmri I'm only going to show up MRI but all the different types have their own slightly different but basically the same in principle kind of interface they look like this more or less and they require you to add a name so let's give this a name like new experiment and you have the option to select your software was used to resent simula if any and then adding events or blocks to Define your experimental design including you know computer files that are used audio files video files\", 'confidence': 0.71540856}\n",
      "{'transcript': \"those are stimuli used and then whether it's post-process or not allows you to provide these other files and support of it so these are designed to be as general as possible to accommodate all different kinds of experiments will allowing you the original researcher to provide as much information is as necessary for someone else to come in later and understand how to reproduce this so it's basically just this form you fill out and as you do so you'll notice it's extensible so if there's software you used that we don't include you can click add new and then select the parent node under what you want to insert this element and then you get a new so if you were using a new kind of neural behavioral system software you could add it here name it and then use that yourself and that way has more people Define experiments will have more pause\", 'confidence': 0.6496672}\n",
      "{'transcript': \"will be assigned after you start your data expected list so now let's go back and cover it we also covered what's web pages you can use to find written information about this and let's go back to our final point of knowing where to get help so we went over this once but it's worth repeating you can either email us or call us at any time for assistance India help at mail that nh.gov is our help desk email address and 301-443-3265 is our help desk phone number available during all normal East Coast business hours so if you have any questions at all or any comments anything like that do not hesitate to contact us we we have a lot of people available and dedicated to trying to make your data harmonization project and the subsequent depositing and sharing of data as efficient as possible so please reach out to us for assistance Soho\", 'confidence': 0.97925788}\n",
      "{'transcript': 'this presentation has been helpful the goal really is to just put you on the right path and get you started and understanding what are system looks like and you know how your data will ultimately you know get set up in order to be uploaded so with that thank you very much for joining me this afternoon and once again please let us know if you need any assistance thank you', 'confidence': 0.66107643}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def :\n",
    "sub_files = range(0,65)\n",
    "\n",
    "full_transcript = \"\"\n",
    "for i in sub_files:\n",
    "    path = './trial/' + str(i) + '_trial.wav'\n",
    "    r = sr.Recognizer()\n",
    "    audio = sr.AudioFile(path)\n",
    "    with audio as source:\n",
    "        audio_file = r.record(source)\n",
    "    result = r.recognize_google(audio_file, language='en-US', show_all=True)\n",
    "    if len(result) > 0:\n",
    "        print(result['alternative'][0])\n",
    "        full_transcript = full_transcript + result['alternative'][0]['transcript'] + ' '\n",
    "    #trying to throw the api off\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello everyone my name is Tracy King and I would like to welcome you to today's Andy a data harmonization webinar a portion of today's webinar will be played using your computer audio this means that if you dial into the audio conference using your phone audio you will hear the video play through your computer or devices speakers rather than over the telephone if you are not hearing the audio properly whether you're connected via the computer or the phone you should check the volume on your computer speakers in or check the device output it's possible that the audio is playing through a different output output like a USB headset if for any reason during today's webinar we experience any technical difficulties we will fried you with a recorded webinar link via email Additionally you can request previously recorded webinars at Anytime by emailing the NDA help desk at NDA help at mail. Nih.gov that information will be provided at the end of the webinar as well just a note that we are not going to answer all the questions at the end of the webinar so please email us any questions that you have and we will get back to you as soon as possible with that we will start the webinar today we'll be going over now to take the data that you are collecting as part of your NIH Grant and harmonize it with the NDA data standard by harmonize we mean take steps to make sure your data are consistent as possible with data from other Laboratories when it is deposited into the NBA system so in our presentation today I'll be going through a slide deck quickly to give an overview of the different pieces of the data harmonization procedure and then I will be going to the actual NDA website in order to give you a live demonstration of some of these features will be discussing and tools and had to approach this so with that let's get started if you have any questions during the course of this presentation please go ahead and type those into the go to client the question section there and we will go over all of the questions at the end so first let's discuss our webinar program in general so this is as I said the India data harmonization webinar this is the second in a series of for webinars we give the first being an overall orientation for new grantees that covers the basics of the entire data sharing process this presentation as I said his own and you did a harmonization this is kind of the step of taking the data you are collecting and preparing it for submission into the infrastructure for sharing we also have a webinar on how to submit the data itself entire webinar just on that and then we also have a fourth webinar on accessing data in the system now all data in NDA are eventually shared for reuse with the research community that 4th webinar is on how to do the set of all the measures instruments data types that are defined in NDA as having a standard structure and your data will need to match that or have a new definition to find to accommodate it so will cover the data dictionary will cover how to set up your data expected list that is a deliverable expected as part of the data sharing process within 6 months of your gramps award and as you as you completed the day is commission agreement which will have initiated your project in giving you control over your collection in NDA it expected list will be the next step in that process that's our primary tracking mechanism for your project going to be covering that today in detail we're also going to understand the data harmonization process so how you're going to approach this process and it is an ongoing process of harmonizing your data to allow it to be submitted share to India maximizing its value for reuse and is always most importantly we will understand how to get help how to find more information how to get the support that you need in order to complete these other steps successfully so given that we will move on to this this / overview of the days during process this presentation will cover kind of the middle steps this kind of linear chart shows you know the overall process from when your you know initially applying for your Grant all the way up until close out and what we're going to be covering now is kind of this second row of getting your good words created to find data expected uploading your data will we actually want to discuss uploading or date ever will you know we're going up to that point we working with the data dictionary to get it harmonize so this presentation is not going to cover the sort of startup points those are in the orientation we're also not going to be covering how to prepare your manuscripts or publish or actually upload the data or the QA process that's all and I recovered in the next women are the submission one so just as a sort of illustration of where we are proud of you oriented in this process so with that let's go ahead and dive in on with the first real sort of subject matter here as I mentioned the gooey this is an abbreviation for the global unique identifier it's the good itself is a series of alphanumeric characters its letters and numbers and it's generated by a tool the tool is a piece of software that you download onto your local computer and then run login and use it to create these identifiers you create these IDs by entering participant information pii from their birth certificate into the tools interface so denture in the least of these data that tool locally on your computer is going to use those to create a series of one-way hash codes those hash codes which are not your from which these data are not deducible is send that's what sent an IH and then that is matched against a database of other kinds of hash code like that if a matches found that existing fluid is returned otherwise a new one is greater than that has returned to you and can be used for that person in this way the data from the same individual in different potential studies can be provided it can be linked across time and space using this ID without anyone ever having access to or sharing any of their personal information to increase the value of all those data it is going to be present in all NBA data as you go mentally a list of these structures which represent are recognized in to find data on surf standard shells each of these structures is itself to list of these elements of each of these elements is a variable or question or an item on one of these instruments the dictionary as a web page and cool also allows you to generate blank template files for submitting it allows you to check all the you know definitions for all the structures it's a powerful resource throughout the entire day shipping process at the bottom of the slide will see you screenshot of the new dictionary search page in real life a little bit later expected list within 6 months of your award and it is something in your projects collection that you would need to go in and Define or have someone to move delegated this privilege to go in and Define it on the website it is our harmonization is constructed so that you know what you going crazy this list we're going to be working with you and that's how you know this process is initiated in and worked out so at the moment we have these two different data sharing the schedules that can be used to determine what dates based on your own projects individual start and end dates there is the first raw data also known as descriptive data and some of our documents and these are they that you're collecting and over the course of your project as it progresses you were depositing them into the system every 6 months during our standard biannual cumulative upload. And then it's your 4 months later after it's been qaid this is. contrasted with analyzed data which are data that are related to your primary aims data that you are generating novelly that will be inappropriate for sharing with a general Community until it is published and that is shared when it's published right away or all the date in your project you should one year after your project end date including all of these data what should be submitted in as soon as possible now I will know that this is changing as of January of 2019 these distinctions will disappear and all data will be shared one year after project end date including the first no-cost extension that is for projects starting effective January of 2020 and you can contact NBA help that mailed NH. Gov for any clarifications about the schedule which is in transition now for some data types this is just to illustrate sort of what I'm talking about when I discuss these different types and it also characterized each of the types and in terms of what needs to be done in addition to the actual date itself what sort of other tasks were required before it can be uploaded based on some of these more complex types complex in the sense of in which they are uploaded to our system so first we can consider dating Michael clinical assessments essentially these are just spreadsheets ultimately when these data are prepared and they are submitted to NDA they will be most likely in the form of a CSV spreadsheet now there are alternative ways of doing it but the basic sort of default process is that it's ESB Spreadsheet will be uploaded these clinical assessment data types are the data that consists of the spreadsheet they don't have a source files they don't have a source e ated additional pages in the website that need to be updated to provide the right metadata and linkages to the metadata so those are spreadsheets then we Imaging and non-functional imaging these are a CSP spreadsheet all the data will have an Associated CSP spreadsheet and this is the standard image of three structure and then it will have these Associated files the raw data files themselves maybe a dicom file for Imaging likewise fmri will have an Associated spreadsheet you will have Associated files with this image 03 spreadsheet which is a dictionary structure for Imaging data and then you also have to define the experiment experiment definition is something in NDA on the website that you will need to create and then linked with your data in that spreadsheet in order to upload it that definition is also required for these other data types that all involve a kind of experiment that needs to be run and the purpose of these definitions is to allow someone to reproduce it as easily as possible and that includes this EG Amex data genomix eight other kinds of Amex data as well as I tracking data then when you see image 03 hear Yeezy Supply one these these terms of each of those and that's where the format is when you make them to recognize as the idea of a data structure and these are those very data structures I was referring to earlier the sort of type specific data structures so the structure EG sub files are one is a data structure generically for all raw EEG data regardless of the experiment you're running or any you know tasks that may be performed during the experiment those are defined in their own structures and the actual data on your scanners are the equipment those are what's in the news 3rd and R Series this is the second in our Series so we're not going to cover the stool today but the two old is what takes in those files and it validates them against the appropriate data dictionary structure to make sure that all of the data within it or matching or harmonize to the standard that you define for your own data within that structure so it's going to check that values are within ranges that are defined that the fields required for upload or not missing and so on and it will make sure that it has valid source files experiment definition when the checks if that's appropriate and then we'll packaging upload the data so essentially this is a post collection free upload keyway check on the data before it's even submitted we have other qhx that are performed after its submitted on those can be discussed in the third webinar free upload a check and you can use this invalidation in submission workspace and then we also have our a data Dictionary tool on our end which is an internal tool that allows us to create project-specific Alias season translations for your collection will discuss with those mean exactly in detail later and as I said at the beginning it's all you can see is available as our email help desk and then 301-443-3265 is our phone number we are available during all normal business hours now we are on the East Coast so if time zones are a factor but these are also available we'll go back to that when I get to the city and we can show those contact information later but for now let's move on to the actual website demonstration phase now that are sort of over view slideshow is complete so now I'm going to show you what the actual dual degree good looks like with addiction she looks like and how you actually use that webpage how you actually go in and create your data expected list and finally I will also be showing you a just an example of what what the data will actually look like in your spreadsheets so first however let's take a look at some web resources so this is the NBA website I'll back up ones you can see the homepage indicated nth. Gov is our website and under our section I'm contributing data you can see we have MD-88 of standards and NDA harmonization approach so we're going to be going over you know how to use the site in this presentation as as kind of a resource however if you're interested in documentation if you prefer the visual learning of reading text we do have these two pages 1 documents on what is expected for each different kind of data kinds of data we've seen before and how it's expected to be dealt with and then we also have a document outlining our general harmonization approach so this covers a lot of what I'll be going over today in terms of what kind of goes back and forth between members of your project and our data dictionary cute curators way with I like this section overall is a good resource for information on date of submission in general and I would highlight these two pages in particular has good resources for information specifically D-Day armatization piece of submission that said let's move over to the website and take a look at the good tool so I have my gujral hears I'm going to launch it and as you see when I click launch it's downloading this file as I mentioned before this is a file you download locally in beer on your system so that we can avoid sending any of the pii to us this does require Java unfortunately that version of java 8 is required in order to run this application on your system so that needs to be installed I'm going to close my console here since you probably won't see that so I'm going to log in with my credentials so that I can use my normal NDA username and password to log into that you can create one of those from the homepage and then you just need to make sure you register for the good tool on your profile page after you created your account now once I accept the warning message I see that will really as it is quite a simple graphic interface for entering the data it's just a basic double-entry interface for each of the fields you would enter in their first name last name we have this answer regarding their middle name if they have a middle name and must be entered here and then as I mentioned before their birthday their sex at Birth and their City Municipality of birth so once you have all these information entered you would basically just could generate go with I'm not going to enter in a lot of information now but you would like generate go ahead and the goo it would appear right here in this little box of this copy to clipboard button which copy and paste it into a document you know where you're maintaining it appropriately so that's basically all there is to creating a single gooey this new button clears the form and then exit closes the tool now if you're creating a time and then copying and pasting them into a document to maintain all there is to know about this however the tool does have a few other features that I would highlight so first the tool can create multiple gueits so to do that I would click first done this good template link it's going to download this CSV Excel file I can as you see here it's populated with sample information different various different historical figures information is in surgery as a placeholder so you would delete all of that leaving this ID the way it is this is just a sequential ID that you might need to fill out if you know have many more people and you can enter in all the day to hear the only difference I would say is this use existing good flag this should be set to yes for everyone in your spreadsheet unless you know for a fact that they are a high-risk hair within this sheet for a false positive so you can also use this tool to create Sudoku words one at a time so is sudoku good witch looks like this this is also very that you know what I normally would look like this but it won't have this_I envy in the middle of it is unlike a good it's just random it's arbitrary it's a totally random sequences of information that has no bearing on the person's VII it's not linked to anything so is sudoku what is appropriate in cases where for whatever reason it's totally impossible to get the data from the in order to create the good for those participants so for example if someone declined to give you their birthday or something like that you can still up there upload their data but you would you could use a Sudoku or if your project you know if it was like retrospective in you contact and then cases like this the suit of wood can be used to use Sudoku aids for cases that are not just went off participants declining to provide all of this information while still consenting to have their data shared you need to email NDA help ml to IAH. Gov to check in about that and you also would need to check in with the help desk in order to get more than one soda go to Once because this will cannot do that so if you need many you will need to contact the helpdesk also be used as a placeholder to upload data for participants while you're you know in the process of getting these data so with that I'll highlight how to convert a pseudo good so when I click converts to toggle Dover to this interface you'll notice it's exactly the same except it has this pseudogout field this is for a promoting a pseudo go into a full Google if data have been uploaded using the pseudo do it and at a later date the actual data required for a full give it is collected now you can also do this in bulk and the promotion template here and he convert multiple Shooters option here will perform that function in exactly the same way that now that will does it for normal Goods generate IDs for all of your participants so if you know but if we are and so the next step will be going into your collect hole is not really going to be an option regardless of what you use some of that I have this text search field I can type in there like an applied that with just a text and that's going to reduce a little bit but there's still you know there's a lot of anxiety has his regulars 44 so I can also filter on different let's say I'm only interested in well just NDA will narrow it down that way in a fly and I guess they're all classified as NDA because I still 44 results in addition to that you can search by category one or multiple selections datatype and then apply our reset filter using these buns so this allows you to narrow down the 3,000 data structures that are currently defined and order to find the one that you are looking for and so how would you determine which one to look for in the first place when you go to create your data expected list it will more or less look like this it will have some dates that were set on this item first of all of one item on it most likely if you're collecting genomic data of some kind will probably have to but at a minimum will have this one will have a research subject and pedigree so this this structure research subject can pedigree which is least to directly right here and are subject to one is it's ID so here's that I hear the data structure page now every project in the every project that shares data through nda's expected to provide this this day to structure this is a summary structure that provides you know just one record per subject information that characterizes that subject and then you know when is this is relevant especially in genomic studies degree information things like that it's a summary structure that are query tools use and therefore is is necessary for everyone to upload it's probably not something you're collecting specifically as part of your project and if we go to the page you can see you know everyone is doing it there's over three hundred thousand participants who have this structure in the system so this is included by default in your date expected list as sort of the seed item it's therefore likely to be the only one when you come in so when there's only one here and it says research of us can pedigree they will actually have a charge enrollment of one probably and then I will have an initial submission date and initial sharing day that are set based on the sort of default schedule for projects and your lord so the dates that you will see here are going to be probably the First Data submission. For your schedule that is more than six months after your award was was given so if you were if your Grant was awarded in let's say let's say December of 2019 and your submission schedule operates on the April October biannual submission periods than your initial submission dates seated in your date expected list would be October of 2020 so in terms of the difference submission dates you should expect to see a July January submission Paradigm if you are getting a grant from the National Institute of Mental Health and you should expect to see in April October Paradigm if you were if you were awarded the grand from and I AAA the Nationals do velka Hall abuse and alcoholism so this is pretty much Mental Health Data expected July 2019 so how do we actually break this list and how does it relate at all to do dictionary that I was just showing you well when I click activate expected I can see I have this dialogue and it prompts me to enter enrollment so let's say I'm going to continue this trend of having originally 100 I'll I'll duplicate the initial submission date that I've seen here and the initial sharing date on just so they're consistent and then I'll search so you can see I'm searching for I was going to search for research subjects of pedigree but that's already on here so let's search for ABC Community books you can see I typed it too fast and it overrode the actual title string so it ever end behavior checklist this structure this is one of our clinical instruments I'm adding that this is an example so I went ahead and added that and you can see now it's on the list I'll go ahead and save just to be safe it should save automatically but it never hurts to save so I can head it this I can delete it once you upload data you won't be able to delete these items anymore but you can edit it in order to change the target enrollment number which is expected to be the final unique count of subjects in that structure the submission date in the sharing day or adjustable as your project progresses so I'll cancel that and then I'll add a new item so I'll add 100 let's give it the same dates just for consistency and let's say I'm also collecting raw Imaging data you can see I search for image and even picked up on it from the data structure search year this list what's getting displayed here is just coming from the data dictionary these are the titles so images the title body image States scales the Title VII SSO one image of 3M riq mo1 these things are the superstructure IDS in a data dictionary so by selecting this I'm indicating hear that I'm collecting raw Imaging data that will be uploaded in July of 2019 and is getting added to the list so okay there we go and once again I'll save just to be safe although it should save automatically know any case you will be adding the data structures that you've identified in the data dictionary as matching or being usable for your project so one way to approach this would be to look at your Grant will be your project and determine you know what you're collecting and go to the data dictionary and find a structure that corresponds to all of the instruments you're collecting all data types you're collecting and once all your data or accounted for you can go in and added to the data expected list know that that would be one way of approaching it is for the simplified fashion what I would recommend that you do instead and the way this will sort of play out the way we sort of expect this to play out is that you can sort of start with those you know those that you can find that a very straightforward that you can identify as being either usable outright or they need to be modified or you can start by identifying ones that don't exist at all obviously and then adding them as new items which I'll show you in just one second but the point I'm getting to is the most important part of this is just get started on adding items today to expected and try to make sure everything's squared away in time for submission in the event that you do have structures that need to be Define from scratch so say you're collecting something and you check the data dictionary and you can't find any example of it or anything even remotely close to me entirely new structure that's also done for the day to expected list of the most common questions we get is how do I Define an entirely novel structure in your system for new daytime collecting well-to-do that you would like to admit it expected and you add it just like these others except that you would click upload definition and toggle to that menu instead of searching but it is structured title let's say I'm still collecting a hundred subjects data and we'll stick with our arbitrary dates just for consistency but again these dates you'll be setting based on your own projects expectations I would provide a title so where to find this from scratch so we'll need a title and then you would pick a file to upload so let's say this file this PDF file and upload here can be a PDF file if it you if we are uploading is a is a question are any of the scanned copy of it including all of the dinosaurs manuals and coding in a PDF file that would be perfectly appropriate it could also be a zipped Archive of multiple files basically you just need to attach here whatever is necessary in your opinion for NDA to go in take a look at it and create an initial draft the structure so this is what we want to see you in terms of new structure definition we have a name we have a file and then add a turkey reader can take that and get started on drafting it out and once they're done with it they'll contact you when you can kind of go back and forth with him to make sure everything works the way you want it to so we've discussed the native dictionary and how to find structures we've discussed how to add those to the date expected list we've discussed how to add new structure requests to the date expected list so before we move on to an actual file and then onto experiments let's take a look at a few structures themselves so that you're familiar with it and then that will help as we go into the actual files so I'm going to pull back up this example that I found of this community checklist so when you go into a structure it will look like this you will have this kind of header section and then the structure itself is just as list of elements so as you can see here this initial block of elements includes his comments element but sex interview age interviewed a thesaurus subject ID which is your internal ID for the participant and the subject ID which is a good of the participant so those five elements are required in every single record of every single structure as the kind of cord identifiers for this record everything else in this structure is is an item or a question in this instrument so you can see we have the name there's a tie provided there's a size provided there's a status of whether it's required or recommended there's a description value range there's a notes column which typically contains the coding for the values you can see that down here here's an example of the notes column being used to provide the underlined values that the code stand for and then there's an alias, so there are a few things I want to highlight here first is the required versus recommended status in this required column a column being recommended just means that if you don't have data in that field then the system will not block you from uploading so the rule for this is that if you collected the data and it's in a recommended column then it must be provided if it's in a required column then the system will technically block you from uploading unless you know if you don't have a value in that field now the other pieces of this year the size the value range the notes where the sort of coding is explained these are all the things that that are real give the definition those are what your defining that's the what constitutes this structure and what the validation tool is going to be checking to try to catch any any mistakes or errors in the use of the standard or or the date it's been entered into these fields now. A couple occasions so far I've mentioned mentioned that I've mentioned translation rules these are two rules you know we have access to that we can use to make your job a lot easier so first I go to Alias he's so in this column you can see there's no Google Earth as an alias of subject key ABC, Minter whr home is an alias for inter whr home so element name so when you upload these data each of the columns in your spreadsheet actually as we discussed this this would be a good time to introduce you to the first two rows to expand this column you can see we've ABC Community into and that matches ABC Community here and to ABC Community O2 these two columns in the first header have to be the two components of this so-called short name the idea of the structure that's how the tool and our system knows which structure this file is so that's necessary second is the row of elements you'll notice that this is basically just the element name column from the definition transposed that's exactly what it is so each of these is one of the elements and let's go ahead and open now one that has data in it and then you'll be able to see what I'm talking about a little bit better so here we go this is fake data and you can see this is basic well it's fake data so this is what it looks like with data in it each of the rows is one subject at one time points record and then each column is an element and in each row for an individual's visit or time pulling it will have the data of the value for that visit or time point so you can see here how it's organized in the spreadsheet ultimately and this also demonstrates what your what your data will look like if they are longitudinal so if you have you know this is a spreadsheet is actually a demonstration of what it would look like if you had two subjects who were each coming in for five different visits each a month apart they have the same ID they have a different date and then their age is increasing the age has to be in months by the way that's why it's 100 and so on so the age and month is going that's matching the time passage and then the IDS are the same and each column contains the appropriate data value for that visit and this is obviously a very simple example But ultimately this is what your data will look like it will be like this in spreadsheets with the subject ID in the data kind of going out in the columns so if you can configure away on your end to have you know those files results automatically from your own internal capture system that that's something that's people found very positive or or you know manual data entry is kind of a guest of the ultimate fallback position so in terms of the actual structure this is what we were talking about so is structural like this with elements and all the different parameters Define is going to be ultimately required for all of the data you or upload SO2 going to go back to our framing here we have been working on the data expected list we've used the data dictionary to identify structures that need to be created from the scratch I've added my request for that to the list I got an existing structures we can use right out of the box to the list and one thing that doesn't happen if it is expected list is potentially edits to existing structures once you have a data curator those requests can be sent to that person directly via email email to NDA help that mailed NH. Gov our help desk address and now we know what our data will you know we'll need to start to look like so now just to wrap up I'm going to show you what it looks like with sort of a more complicated kind of data with imaging data and then I will take a quick look experiments and then that will wrap up the live presentation so to do that I have my image 03 spreadsheet prepared and as I said before you know even these other types they're all going to have a CSP spreadsheet like this this one is just to be pointing to other files as well so here we have the same kind of spreadsheet the elements out here in these columns and then you can see this is the same person coming in once per month five times in a row so in in most respects this is exactly the same as the spreadsheet I was just showing you the date and just need to be populated in these columns the key differences are associate files and experiments which of these two columns so first let's deal with Associated files so this has a call the image file similar structures and by similar I mean the other structures that are type specific file metadata structures like EEG subject and sell on d-sub files rather they also have a file type element like this and what goes in that element is not data you collected but it's actually passed as file in question so subject X here in their visit on January 1st 2009 at cetera had these cans in these files taken and then you know there's data about the rest of the files Android rest of this spreadsheet so you can zip up all the scans that were done by one person under the same parameters of the same day and upload element there are ultimately more there more file elements you can use to add more data up to up to summon number of individual files prefer not to zip too many of them and then they need to be located in the directory on your local system the same system as this file and then the tool as it parses this spreadsheet will automatically go and identify those files and get them so you can see it here I have really the absolute path starting at the drive-thru and then going up through my user folder into this demonstration folder named intro 3 you'll notice this duplicate exactly the directory structure I see when I go hear my demonstration folder and then this image. PNG is of course exactly the file on referencing Peter and obviously you know if you're uploading a zip file full of diatoms for this person that you know it's not going to be this simple P&G but this this is the principle underlying the organization of this this must take a path to the file in question you can be an abscess past like this they can also be a relative passed starting at the location the working directory of this csb spreadsheet itself as when it goes into the tool but you can't go wrong with an absolute path like this and once those are populated correctly the tool will check that if it fails to find one you'll you'll know and you'll need to be able to it you need to make sure he's at the path is correct for the you know the directory structure is correct so that's how the files are uploaded it's it's basically just as simple as getting the path in the file type element and then let's take a look now and experiments so require an experiment only functional MRI does and then the other experiment types do like EEG you know makes sense so on so for that reason experiment ID here is going to be empty for those and in this purported fmri record it will be filled in you can see the scan types different that's how I know that's fmri so let's take a look at image of three you can act as a sort of navigation Aid hear these little eyes can show you links directly to the structures reach of your data expected items so here is image here's my image file structure and here's my experiment it so this is obviously well it's conditional so a few of these are conditional and those go into these logical operators and if the operators Matt then it becomes required and if not then it's it's not necessary so when the scan type is fmri instead of us can type is another element here so scan take to be any of these recognized Imaging format so this can type is fmri then an experiment ID must be provided so let's go back to our collection and take a look at the experiment stabbed which is those are Define so first I would click add new experiment attic new experiment you can copy existing experiments as sort of a base to start from in events where the experiments very similar so let's stick with fmri I'm only going to show up MRI but all the different types have their own slightly different but basically the same in principle kind of interface they look like this more or less and they require you to add a name so let's give this a name like new experiment and you have the option to select your software was used to resent simula if any and then adding events or blocks to Define your experimental design including you know computer files that are used audio files video files those are stimuli used and then whether it's post-process or not allows you to provide these other files and support of it so these are designed to be as general as possible to accommodate all different kinds of experiments will allowing you the original researcher to provide as much information is as necessary for someone else to come in later and understand how to reproduce this so it's basically just this form you fill out and as you do so you'll notice it's extensible so if there's software you used that we don't include you can click add new and then select the parent node under what you want to insert this element and then you get a new so if you were using a new kind of neural behavioral system software you could add it here name it and then use that yourself and that way has more people Define experiments will have more pause will be assigned after you start your data expected list so now let's go back and cover it we also covered what's web pages you can use to find written information about this and let's go back to our final point of knowing where to get help so we went over this once but it's worth repeating you can either email us or call us at any time for assistance India help at mail that nh.gov is our help desk email address and 301-443-3265 is our help desk phone number available during all normal East Coast business hours so if you have any questions at all or any comments anything like that do not hesitate to contact us we we have a lot of people available and dedicated to trying to make your data harmonization project and the subsequent depositing and sharing of data as efficient as possible so please reach out to us for assistance Soho this presentation has been helpful the goal really is to just put you on the right path and get you started and understanding what are system looks like and you know how your data will ultimately you know get set up in order to be uploaded so with that thank you very much for joining me this afternoon and once again please let us know if you need any assistance thank you \""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_vid = 'vid2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(current_vid, \"w\")\n",
    "text_file.write(full_transcript)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
