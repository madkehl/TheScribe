so my name is Malcolm Jackson I'll be giving your business presentation today you should only be able to hear me and see my screen I'm showing me and I am hd8 archive homepage and today we're going to be discussing how to validate and submit your data into the NIMH data archive as part of this current ongoing submission. Now our Standards Commission periods are either January 15th or July 15th if you're an NIMH awardee or April 1st to October 1st if you're an anion AAA award Authority so at the top here the first thing I'd like to show you is some of the resources that are available on our website as a as a reference tool or you going forward or contact information and so on then we will go into some of the assumptions of this presentation some of the store prerequisites to submitting data that at this point if you were planning on submitting during the current. Will will need to have already been taken care of and if not should be taken care of as soon as possible and then I will move on to a demonstration of the actual tool that is used to upload data to the system and to validate your data against the standards as they're defined as they were defined by you in the NIMH State archives data dictionary and then we'll discuss a little bit what happens after you upload data as I said first let's take a look at the web pages we have that provide written information about this you can reference so they contribute data section here on our home page is where you can find all of our documentation on the expected data standards the approach to harmonization as well as information on post submission quality assurance checks these automated checks be run after your data are uploaded and then the header here of the section provides a good overview of the overall process including how you plan for and then execute a date of submission in addition to that we have a page with information on the tools as I mentioned before the validation upload tool is the key tool that you will be using to validate and then upload your data this is an HTML based tool essentially that just means it's it's a web page using your browser there's also a version of the tool that's a python package you can use upload data if you prefer that but the basic or default workflow that most people will be using is going to involve the HTML validation tool so we're going to cover that in particular today we also have and then the 4th webinar is on how to access data now that refers to accessing data for secondary analysis on for your own research so that may not be necessarily applicable to those of you on this presentation but of course encourage anyone to reuse data I'd look into reusing data as part of the project so when that's scheduled that may also be of use and then of course the very general information is available under our about us page including their contact information I want to mention this at the top because we put a lot of resources into being available to provide active support throughout the process so if you have any questions at all or any feedback or any uncertainty whatsoever about any of this don't hesitate at all to email us at NGL that mail that NH. Gov or to call us on the phone at 301-443-3265 we are available during all business hours on the east coast and as far as questions during this presentation are concerned again I encourage you to reach out to us directly at this email address or this phone number if you type in your question to the gotowebinar chat window during the presentation we will try to go over all of those together at the end and we will end the webinar right at 3 today so if there are questions we don't get to you can send those to this email address once again that send email to mail NH. Gov or call us at 301-443-3265 let's go ahead and move into the first part of our presentation I mentioned which is on the sort of prerequisites are assumptions about your projects now when your project get started there are two things the initial emails you get your introductory emails will Avast you for and those primer fire Grant throughout the entire duration of that grants effective dates spot if one has not been completed yet I recommend that you do that as soon as possible that's generally expected within six months of award now the second thing that is expected within six months of award is the data expected list in your collection now in terms of the overall process the data submission agreement really is what gets things started and once we receive that the principal investigator is given ownership of an indie a collection now it's election is this virtual container on the website into which all of the data associated with your project will be uploaded so after you've returned in the agreement this is turned over to you and then you're able to use this to start getting set up now that primarily involves creating the date expected list and now before I go into this in a little bit of detail you might be wondering what page this is now I would expect many of without admin will be sufficient for anyone who's just going to be uploading data and doesn't need the ability to assign access to people and then it's important to note that anyone with access to the collection like this will have access to any research data you upload into it and when you upload the data to the collection initially only those people in that permissions Tab and NDA staff and of course the NIH program staff will have access to it so it's not it's definitely not made available to the general research committee at that point so they expected list which at this point should be completed for you so this should be a review but just in case it's not this needs to be done as soon as possible and it is basically a list of all the data you're collecting and then it's going to be tied to data structures in the data dictionary proceeding webinar in the series so if you have no idea at all what I'm talking about and you never seen this before the thing to do will be to go watch a recording of that with an r and then go into your collection and set this up if you have not started this it will probably create a delay in your submission because I'm as covered another webinar this is basically the message through which you insure your data that you are preparing to deposit are consistent with a defined data dictionary structures and which all data are required to have so you can just upload data and whatever structure of format you know you happen to have on hand at the moment and needs to be harmonized to a definition in the system that will validate it against that definition which you create in conjunction with NDA staff here the date of curators and Vac does take time it takes time to set that up so if you haven't started that you may not be able to submit until completed and there may be a backlog of that so I would anticipate a delay in that case so those are the things that if they are not done already should be resolved urgently and to finish up what is sort of a prerequisite let's take a look at the good tool quickly now goods are these alphanumeric IDs and they correspond to an individual a subject or participant in your research project and they will need to have been created or you can create them now using this tool which I've just downloaded from the tools menu hopefully you got that so I'm going to log in with my normal NDA credentials and a Java version of java 8 and you can see it's this basic data entry interface this is also going over in detail Nevada harmonization webinar so again refer you to that hey everyone just bear with us for a moment see what's happening here hi everyone so if Malcolm's computer cannot become back online we have a couple options we can provide you with a link to our most recent validation and submission webinar which goes over the exact same information it's just a pre-recorded webinar or hopefully he can get back on he just messaged me that if computer died so welcome to live webinars anyway give us it give us a minute if it doesn't come back on what I'll do is I will email all of you with a link on how to get onto and watch at your leisure the same webinar sorry for any inconvenience so I'm going to right now we're waiting on Malcolm still but I'm just sending you a link where you can find a copy of all of our webinars are previously previously recorded webinars what you would do as you log in just like you do for registration and then you can select which webinar you'd like to do I'm just getting the web address now so I sent you a link in the general chat it says it's our stage where you are stage Channel where you can find a copy of all of our webinars again hopefully Malcolm will be back in a moment a little bit awkward but hopefully we can get this back up and running if not please chat in the question tab of the webinar if you can see the link that I just provided to its www.stage.com 4-channel for NIMH data archive can see it and I really apologize I'm going to send out this link to everybody via email as well if you can hold on for a few more minutes I'll be great and we're just crossing our fingers if not we'll see what happens he's still trying one of these are fast was a link be available for a while this link is always available and if you want to request a copy of any of our webinars that we've given from the orientation webinar of the data harmonization and the doubt validation and submission the ones that he talked about her all available at this link so I'm trying to see it does not matter where you're accessing it from you just need to register as if you are registering for one of our webinars and buddy can register you can share it with your colleagues as well if you haven't taken one of their first webinars the orientation or the harmonization you should probably watch those first this is the third one priority announced that you would take prior to submission which is coming up so you could be all caught up that way as well Malcolm saying he should be back in just a second he's texting me frantically so hopefully we'll see but we'll see I think he may be back Malcolm can you hear me you are 40 very patient people that are attending this we thank you hello can everyone hear me now yeah I think we're good okay thank you sorry about that everyone that looks like we did have one of these Network earthquakes I was mentioning at the top so thank you for bearing with us so it sounds like you can hear my see my screen and hear me once again once again sorry about that so let's get let's get right back into it now that my now that my networking is back online so as I was saying the good tool which you can still see right here in my screen is the java-based program that you need to download onto your computer and then run and then entering all of the personal information for your research participants in order to go to glue it you can also use this to create Sudoku Ed's I'm just going to do that now so you can see you sort of generally what a good looks like this is what they look like in general and that is what you would use as the IDS or the primary keys for your participants as you're uploading data again this is covered more in the data harmonization webinar but I do like to mention these things in this webinar to make sure that no one missed any of the earlier steps as they're getting ready to try to upload so you don't get to the tool and wonder why it's not letting you deposit the data so now let's move on to how you actually upload data so first let's take a look at the data dictionary and let's also open up okay so we're going to our data dictionary here where all of our data structures are located so down here at the bottom of this request help this opens up another pain where you can enter in your information your collection ID and a message now if you need to contact the help desk while you're using the validation tool there is no better way of doing that than to do this and to fill this out and submit it so when you email us that's very great and well and good if you use this tool to contact us this actually it does send us your message and it also includes automatically a variety of troubleshooting information that we can use to help address your question a lot faster than then that dispenses with us having to ask you for that troubleshooting information and you having to provide it so definitely use that if you need to submit a ticket during your time in the validation and upload tool so let's say that you have all your good words created your data expected list is complete your data files have all been loaded with your data you've turned in your submission agreement and everything is just ready to go so to proceed from here you would click create new submission and you can see here the the sort of work clothes at the top is going to be to validate the data there will be a step to associate files and that will be relevant for people who are submitting not just the spreadsheets to CSV files and I'm about to show you and just a sort of backup and and and frame that the day that you'll be uploading using this tool will be CSV tables all the data gets uploaded to and ye will be in a CSV template and those are what gets pushed to our system through this tool and then some types of data like Imaging a genomics files EEG data on those kinds of things will have the associated raw or underlying data files of some kind and those also have a CSV spreadsheet that links to those files on that step is where we will link the files directly and then push that and then it builds a package and uploads to data so let's take a look at some data files and start validating them so I have my demonstration data folder here that is a variety of things in it so I'm just going to I'm going to take that and I'm just going to drag all of them into the tool just like that so that's how you get started and really this is this is this is really all there is to it now the core take away from this is going to be that the tool itself provides you the feedback and the information that should be necessary in order to identify the problem hopefully resolve it but if not at least identify it and then perhaps determine the appropriate course of action to resolve it soon as you can see here I've I have three entries now in my tool even though I dragged into sub folder and all these other files see what the tools doing it it's automatically ignoring anything that's not a CSV file because that's really what the tool grabs and wants to validate and submit so these are this year's fees it's identified them as potential data files based on the fact that they're csps and then it's identified them as being a certain data structure that's what the short name call him indicates now from this point I can proceed or I can continue to others another there's a big request help button that's how you request help and then you can validate all the files you have at once time in hundred different packages it's totally irrelevant as far as we're concerned all that matters is that all of it gets uploaded in that all of it is validated so from this point you can see we are these are bolted out there complete with errors it's displaying errors it's displaying some number of warnings and then I have the option to revalidate to I revalidate it's just going to you know it's going to get the exact same result today that I did last time so let's open up one of these files let's this one has a huge number of Errors so let's open well I got ahead of myself are so actually we don't need to open the file we can actually see the errors right here in the tool by clicking on the table and then it drops down and in this second viewing pain we can see the errors themselves you can see there's a lot of Errors there's a hundred and seventy-five of them in fact play I don't want to see the errors I only want to do the warnings I can on check errors and just display warnings I can also check both my lights on check warnings and then I can click Group by error message here and that's going to collapse this into the types of errors that I have and then display the record level errors below that so what exactly am I looking at here well this is a list of the problems that the tool is detected in this particular file and I have this file I'm going to open it now right here and when I say problems that's that's really what the tool is referring to was an error so an error represents a place where your file and the NDA data dictionary structure that this template is identified as being are not consistent so or in some cases like this there are others are two general standards that need to be mad that aren't necessarily in your data dictionary structure but they're consistent across all data dictionary structures so if I believe if I Collapse these I'll have just these two types of Errors there's a lot of this invalid range error so that's taking a while to collapse but those are really the only two kinds so this means that so here's the message telling me exactly what it's finding and then consistent throat all are types it's going to show you the record number it's going to show you a column name and then it's going to allow you to hide these as you kind of go through them to clear things out so we could even just hide this entire group The invalid range error is going to be indicating as it says the data fields must match the value range fields and noting that texts are case-sensitive in value range so let's take a look at the data dictionary structures I'm going back over to my data dictionary back on the NBA website and I'll search for Willow search for ABC since that's the name of this year so so so sure that I was just looking at the shore you can see the short name here is ABC_Community O2 that is the short name here and if we go to the file you'll see that that's also what's here in the top row so this is my fake data sample file this is the file whose results are displayed on the page behind me there so ABC community and then two in the first two columns in the first row the Tule Reeds that and that's how it knows that this is the data structure against which to match your file so when you load this into the tool it's taking that ID from the first header row and then it's taking the second header row and it's parsing that to identify which column is which element and then it's going to look at the data dictionary and compare each of those elements to the definition sooner subject keep calling which is a good that's our first one here there were showing this fake good whoops and this is what it's checking to see if it's a valid good or not so when the tulsi's that this element is first it's a datatype good in those that means it's one of our goo addendums to check to make sure they're all valid so it did only and found that this is not a valid do it so you can see this probably isn't valid good since it past validation and then the so you know what you're worth the record number is going to be the row number so you can see and record 6 7 8 9 and 10 we have our bad do it that's calling them subject key so the tool in sequence is telling us which robots in which column it's in and then by the category we know like what the issue is so in this case based on the category we know the issue has to do with his false or bad so in the invalid range you can see that you know 12345 these weed eater by record and it's the same elements over and over again so there's some set of elements in my file where the value range is just not matched and I think that's going to be right here you can see these interrelation parent that was the first one in the sequence and then yes no is the value range and then you can see those are those are actually right here they all have no numbers in them so this is not matching the range it's not even the same type of data so the tool is detecting that and basically perceiving that as a problem so the solution to that might be that you've made a data entry mistake somehow and it needs to be corrected or the explanation might be that there's something about the data dictionary that needs to be updated to accommodate your data so in the ladder case an example of the latter case might be if this is a good example because we have yes or no and if you click on this little I this is all the this is you supported translation rules currently in place for this element and the translation rule is basically what tells the system you know we are default standard-definition says yes and no with a capitalized letter they can also take however Y and lowercase or uppercase and translate there so that basically allows you if you have one of those rules implemented to leave your data in a different form and then have the tool automatically parsec correctly and interpret it correctly and rules like that that's a transaction example of a translation rule in which value on your end is interpreted as a value as a different kind of default value in the dictionary and the other kind of rule you would find is an alias where your element names are different and we Implement an alias rule that allows the tool to interpret your element name the correct element name for that element in that structure for you so those are set up with your data curator as part of the day or harmonization process so if you would know who you're dating. Raisin you need a rule like that implemented in order to get one of these errors resolved or you know you know you're going to encounter it based on what the data structure says you can go ahead and request that and and have them help you set it up so it is a kind of example of this you know if you were collecting this instrument and you were using 0 and 1 as the coating instead of yes or no you would need to have a translation rule implemented and then your data could still say one or zero like these do but the tool would recognize it and interpret it based on that rule now one thing about that and know is that if you have any rules like that setup you need to have this box checks to validate against a custom scope so this custom scope refers to this this idea of the data dictionary being customized with war the scope of your specific project so when we have a rule like a translation rule or an alias those aren't Global rules that affect every single person who uses ndir submit stayed at their specific to your collection so you would need to validate against a custom Scope when you do that it will ask you to select your collection ID and then the tool will look at your specific data dictionary rules and use those to validate so that's how you would Implement that in the tool once the rules are set up on your end now let's move on from this file and I hope you know I hope that the the sort of main takeaway cuz these are not even remotely close to the only kinds of Errors you can get does it take away from this should be that when you drop a file in here the tool will tell you where in the fire all of the errors are and what the error is and then you need to determine whether that's either a mistake in the data or if it's a mistake in terms of the harmonization between your data and the dictionary and in the former case fix it and in the latter case talk to your data curator to see what the best solution is and if you if that's all you get out of this presentation that will be that should be sufficient to get you well on the way to getting your data submitted so I have the luxury of just being able to remove that entirely Let's ignore that with a few hundred errors and move on to these two other files and we'll take a look at two other kinds of errors in the same principle but let's take a look at these other files as opposed to be sorted just general wide-ranging Arizona close this and that will basically I'm doing now is I'm just going to show you how easy it is resolved one of these problems obviously that's very simplified so now I'm looking at the end our subject file this of course is the research subject and pedigree data structure let's go to our I'm going to back while my okay now I'm going to look up that structure hopefully it's clear that the data dictionary is a very useful resource for helping troubleshoot any problems you see in your file just because that's where you know that's where the definition is that's what the elevation was checking against so here I have a single are there 62 warnings actually let me mention that now which I didn't before so an error as I said represents a place where the validation tool finds a discrepancy between your files and the data dictionary a warning represents practice that's been violated like the removal of a blank column or something like that so so those are issues that could put really contribute to problems at some point but they don't actually cause any errors and they don't actually block your technical submission so warnings can safely be disregarded in most cases so you can go ahead and ignore the warnings in your files if you so choose so this file only has one error and you can see the air is not an integer so that's already telling us exactly what the problem is the column name is interview age the record number is 5 and the messages at the field provided was not an integer that seems fairly straightforward and indeed if we go here you can see we have our column interview age Record 5 you'll note that the Record 5 corresponds to roast 7 in the file because Rose 1 and 2 or the headers certifying the row in your file just add to the record number and you can see here that it says 12 it's a string I'll just change that obviously that's not exactly 12 but it's correct based on this age sequence in this fake data so I just corrected it and I just save this file and then if I go and click revalidate there now you can no longer bold 0 errors so this file is now ready to upload so if I remove this offending file with are still it would give me an error-free sort of you here and I would be able to proceed with a package to upload the first let's resolve this other error so you know you may have a lot more data you may have real data instead of the simplified fake data that I have but in the the general principle is is the same which is that you use the tool to troubleshoot errors in the data identify their cause and then resolve them one way or the other so let's look at image spreadsheet now now this is important to know those first two files I showed you or just you know they're just questionnaires or sort of basic structures that do Sew It ultimately those eight or just to be a spreadsheet what we're looking at here is an instance of our image structure image of 03 is the ID of this structure and this is the basic sort of General structure for all raw Imaging data of any kind so a lot of you may be uploading Imaging data you may be using this and the spreadsheet itself basically contains metadata on the files you know there's a lot of elements in the structure that don't go all the way out and then most importantly I guess since 4 for our purposes here at least it's important to note that these structures are also how the tool goes and gets your actual underline file so you can see here in Otay comms are typically what we expect from Imaging there may be other file types for other data types but as far as this is is going it's going to be the same sort of basic principle for all these data types they are types with Associated files like this which is that one of the elements in the data structure will be a file type element like image file may also be called Data file and some of the structures and all cases that element will take the path of the file or zipped-up archive of files that are associated with the record who's metadata is in that row so if subjects year came in for you know what we're looking at here is a subject who came in for five visits once every month for 5 months you can see our dates are matching monthly the interview age which is in months is increasing monthly and we have our image file element which is providing the actual path to the files that were collected at each of these visits I'm under these parameters of the same visit but you've sort of different kinds of Imaging those are two rows but if you have one visit where they have a lot of images in the same exact setup then you can sort of zip close together and upload them all under the same record so here you can see I'm pulling onto these paying images just as a sample and I'm I'm I'm showing the absolute path from the the drive route all the way up to the subdirectory where the file is located and if I open back up my demonstration folder you can see it's going to duplicate this and then my image 03 CC users my name desktop demo image of 3 so here's that are png's the very P&G that I am pointing to and if I go back to my tool it's telling me that this okay actually sex is null so this is required to Camping no missing required field records 6 I know that's row a and indeed right here so I just need to fill that in I'll save but I'll leave that file open for later and then when I revalidate it's past so obviously this is once again pretty simple but this is how it's done so now to move on to the next other was an error message after the top it went away so it must have been a temporary problem so staged so you can see here my and are subject has 0 files required there are no Associated files with my image structure does have six required files now as we go to this you'll see it's found all six of these files and you'll note that it did that even with this path not being exactly correct so what located that will it actually just okay here's what it's doing it's just identifying that okay so I upload it okay so sorry this interface has changed little bit since the last time I used it but it has that it's Match 6 out of 6 so it's going to let me build a package because I had those files in the same directory as showing the same directory as I have my image 03 it's scanned it and all its subfolders and found these exact match filenames irrespective of the past so so little clear serve account of this the path in your CSV file should be the exact absolute path of the files current location I'm at we just observed when you're trying to do relative pads another tool is designed to accommodate for that and it does it by just searching on the file names within the working directory and all of its subdirectories so that's sort of got interpreted as a relative path because it's in the same working directory but the best practice is going to be to use an absolute past to their direct location I will also allow you to drop them in from a non you know the different a different directory if that's necessary okay so hopefully that was clear a little weird with that so now that I have that setup in my package is ready to get built I'm going to login now you might be noticing now that this is only the first time I've logged in on up until this point I have not logged in at all so over the course of your project you know you can be validating this and every day every time you add to a datafile you could just validate it and then and make sure that it's remaining consistent with the standards you've defined and an only when you actually go to push data into NDA will it ask you to login and authenticate with NDA in order to allow you to pull your project information and build a package so after you done validating when you're ready to push a package of any size once again you could do this one file you could do this with 10 files or million files it's irrelevant once you're ready you'll see this page and it will list all the files you have it will include the count of associated files my 6 awkwardly Associated files are right here they're all accounted for and it will create my submission package so now it's ready to go a mile at 0% so after I check this to verify that nothing I'm uploading contains personally identifiable information I can submit this once you submit this data now let's move on to what happens next and I know where a 250 we lost little time with the network problem but let's kind of briefly cover what happens at the end of this process what comes next and then we should have time for a couple of questions so after you click submit you'll start seeing progress at this point and only at this point if it gets interrupted and by this point I mean clicking submit data only after you click submit data are you going to be able to come back and resume an in-progress submission so once you click submit that's when I initiate sit and starts the process and you can come back and resume it so this may take some time depending on how large your packages and may happen very quickly this is a very small package so I suspect it would be quite quickly yours might be significantly larger and might take hours to upload and once it's completed you will get an automated email notification that says we received your submission at that point for that submission in and of itself you're basically completed you're done so you can go to your submissions tab in your collection and the submission to appear this is an empty collections there's nothing here but yours will be listed in this tab that's the one I was just looting to the list what has been uploaded and when with its name and so on you'll be able to use that to verify your submission was received and at that point you won't hear anything from NDA for a while it might after four months before you hear anything or a couple of months and what happens after this is gone over let's go to contribute data or general information section on contributing and take a look at our data keyway reporting step so after your uploaded over the following months there are a series of automated quality checks that are run on the data and these are why you know the main reason why it's important to what one of the main reasons why it's important to upload cumulative datasets you know for for those types of dated that call for it so for example you don't need to be uploading the exact same duplicate dicom files every 6 months but it is good to re-upload a cumulative dataset every 6 months of just kind of the spreadsheet data and that allows this kind of comparison between cumulative datasets take place these kind of post submission Jack's so you can see this page for a list of the checks we run and after a few months once it has finished you may receive a notification from US informing you that we found errors in your submission now that might be you forgot to upload cumulatively and we detected that and we are asking that you do upload a cumulative day is sat or a might be some of these issues you can see you interview age and date consistencies or check two Sexes are checked for consistency just in case good source subject ID mappings or checked and so on so is an example of this lipstick with her fake data again in our image as I said these are rows of the same subject they're the same as expected to have you uploaded data where someone one month later was 10 years older we would detect that and if you don't do your question answer or you think of one later email it to us or you can call us so let's see the first question is do we need to notify our data curator if we plan to use pre-existing aliases and place of the data element name listed in the data dictionary so I would recommend to make sure it will work if you already have one in your working with them so if they're already listed in that dictionary page those are older Global Alias he's from before we implemented the system that allows us to do it on a project basis so I don't know the answer to this question for sure but I it's possible that those are not automatically included in yours in your specific scope for it so I would check with them that is that is just the best thing to do in general when in doubt this weather you can use this at home so that IP address should not make any difference for anything on the website or for accessing the webinar recording another question here we did not collect the middle of a name or the place of birth of our subjects this is going back to that data that's needed in order to create a good is there some kind of exception we can request that we don't need that information in argue it so an actual do it a full goo it cannot be created without all of that information however in a case like this the sort of default prescriptive course of action would be to take all the data that you have that is applied to it sounds like it's all your data so you would still upload that you would be able to use this which is a totally just random arbitrary ID and you be able to so hopefully that answers that question for anyone else running into it so the next question is for the 15th for the for for data submission deadline so for example January 15th are we required to submit all data collected up until that date or is there a cutoff point so obviously if your if if someone came in for a clinical visit on January 14th you're probably not going to get that person's data collected at that visit into the upload by the 15th so there's submission. You know the one that ends in January basically goes from December 1st to January 15th so in terms of a cut-off point we would give you know did that December 15th date as the cutoff point so if you want to upload everything collected through November and then stuff your collecting in December and January if you're collecting data during during the submission. That can be kind of rolled into the next. Data set in order to make sure you have enough time to clean and enter those day that you're collecting right before the deadline so that that's kind of the basic cut off that I would give okay so we have two more questions here you for the first one as you mentioned the special instructions for uploading Imaging data are there assembly instructions for uploading ecological momentary assessment data so I'm not familiar with ecological momentary assessment data that is something I would ask your data curator they may need they may need different structure for it there may be an existing structure for it that I'm not aware of already so they may have everything they have an answer they can give you confidently right off the bat to that so that's something I would I would reach out directly to us offline for the second is is there a recommended data but you do not have any dated input for a subject you leave it blank the answer to that question is yes so if a data field in a structure is marked as recommended as opposed to require can you just didn't collect it you just leave it blank if it's required and you just didn't collect it there will be a missing code most of the time that's 999 but it's specific Indian with there's different ones in some different structures so that's something you would want to check into dictionary but if it's not required and you didn't collected just leave it blank if you didn't collect it then you should upload it so that covers an hour and we don't have any additional questions at the moment so hopefully that was helpful for everyone again thank you very much for bearing with us during the brief technical outage we had and if anything else comes to mind or if you didn't get a chance to ask your question right here at the end go ahead and email sfmta Health at Mathilda NH. Gov or contact us on the phone at 301-443-3265 I'll also at the Disco Banner was recorded a recording of 