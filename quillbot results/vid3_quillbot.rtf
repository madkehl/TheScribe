{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red28\green28\blue28;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c14510\c14510\c14510;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Malcolm Jackson: We're going to be discussing how to validate and submit your data into the NIMH data archive as part of this current ongoing submission. Standards Commission periods are either January 15th or July 15th and April 1st to October 1st if you're an anion AAA award Authority. The validation upload tool is the key tool that you will be using to validate and then upload your data. This is an HTML based tool essentially that just means it's it's a web page using your browser. that's a python package you can use upload data if you prefer that but the basic or default workflow that most people will be using is going to involve the HTML validation tool\
\
The 4th webinar is on how to access data now that refers to accessing data for secondary analysis on for your own research. I encourage you to reach out to us directly at this email address or this phone number if you type in your question to the gotowebinar chat window during the presentation. We will try to go over all of those questions together at the end and we will end the webinar right at 3 today so if there are questions we don't get to you can send those to this email addresses.\
\
The expected list is a list of all the data you're collecting and then it's going to be tied to data structures in the data dictionary structures. Anyone with access to the collection like this will have access to any research data you upload into it. If you have not started this it will probably create a delay in your submission because there may be a backlog of curators and Vac does take time to set that up so if you haven't started that you may not be able to submit until completed.\
\
Malcolm's computer cannot become back online we have a couple options we can provide you with a link to our most recent validation and submission webinar which goes over the exact same information. If it doesn't come back on what I'll do is I will email all of you with an email link on how to get onto and watch at your leisure the same webinar. sorry for any inconvenience Malcolm will be back in a moment a little bit awkward but hopefully we can get this back up and running if not.\
\
Hello can everyone hear me now yeah I think we're good okay thank you sorry about that everyone that looks like we did have one of these Network earthquakes I was mentioning at the top so thank you for bearing with us. The good tool which you can still see right here in my screen is the java-based program that you need to download onto your computer and then run and then entering all of the personal information for your research participants in order to go to glue it.\
\
The tool itself provides you the feedback and the information that should be necessary in order to identify the problem hopefully resolve it but if not at least identify it and then perhaps determine the appropriate course of action to resolve it. Some types of data like Imaging a genomics files EEG data on those kinds of things will have the associated raw or underlying data files of some kind. Those also have a CSV spreadsheet that links to those files on that step is where we will link the files directly and then push that and then it builds a package and uploads to data.\
\
Tool automatically ignores anything that's not a CSV file. 100 and seventy-five of them in fact play. I don't want to see the errors I only want to do the warnings I can on check errors and just display warnings. I can also check both my lights on check warnings and then I can click Group by error message here and that's going to collapse this into the types of errors that I have and then display the record level errors below that. It's just going to get the exact same result today that I did last time so let's open up one of these files.\
\
There's a lot of this invalid range error so that's taking a while to collapse but those are really the only two kinds so this means that so here's the message telling me exactly what it's finding and then consistent throat. When you load this into the tool it's taking that ID from the first header row and then the second header row. It's parsing that to identify which column is which element and then it's going to look at the data dictionary and compare each of those elements to the definition sooner subject keep calling which is a good.\
\
A translation rule is basically what tells the system you know we are default standard-definition says yes and no with a capitalized letter they can also take however Y and lowercase or uppercase and translate there. The other kind of rule you would find is an alias where your element names are different. These are set up with your data curator as part of the day or harmonization process so if you would know who you're dating.\
\
Let's ignore that with a few hundred errors and move on to these two other files and we'll take a look at two other kinds of errors in the same principle. When you drop a file in here the tool will tell you where in the file all of the errors are and what the error is and then you need to determine whether that's either a mistake in the data or if it's a mistake. In the former case talk to your data curator to see what the best solution is and if you if that's all you get out of this presentation that will be sufficient to get you well on the way to getting your data submitted.\
\
An error represents a place where the validation tool finds a discrepancy between your files and the data dictionary. A warning represents practice that's been violated like the removal of a blank column or something like that. Those are issues that could put really contribute to problems at some point but they don't actually cause any errors and they don'st actually block your technical submission so warnings can safely be disregarded in most cases so you can go ahead and ignore the warnings in your files if you so choose.\
\
In the the general principle is is the same which is that you use the tool to troubleshoot errors in the data identify their cause and then resolve them one way or the other. This is the basic sort of General structure for all raw Imaging data of any kind. The spreadsheet itself basically contains metadata on the files and the structures are also how the tool goes and gets your actual underline file. The data structure will take the path of the file or zipped-up archive of files that are associated with the record.\
\
If you have one visit where they have a lot of images in the same exact setup then you can sort of zip close together and upload them all under the same record. The path in your CSV file should be the exact absolute path of the files current location I'm at we just observed when you're trying to do relative pads another tool is designed to accommodate for that and it does it by just searching on the file names within the working directory and all of its subdirectories. The best practice is going to be to use an absolute path to their direct location.\
\
NDA will ask you to login and authenticate with NDA in order to allow you to pull your project information and build a package. Once you're ready you'll see this page and it will list all the files you have it will include the count of associated files. After you click submit that's when I initiate sit and starts the process and you can come back and resume it. This may take some time depending on how large your packages and may happen very quickly this is a very small package.\
\
After your uploaded over the following months there are a series of automated quality checks that are run on the data. At that point you won't hear anything from NDA for a while it might after four months before you hear anything or a couple of months. It is good to re-upload a cumulative dataset every 6 months of just kind of the spreadsheet data and that allows this kind of comparison between cumulative datasets take place. After a few months once it has finished you may receive a notification from US informing you that we found errors in your submission.\
\
I would recommend to make sure it will work if you already have one in your working with them so if they're already listed in that dictionary page those are older Global Alias he's from before we implemented the system that allows us to do it on a project basis. The one that ends in January basically goes from December 1st to January 15th so in terms of a cut-off point we would give you know did that December 15th date as the cutoff point. If you want to upload everything collected through November and then stuff your collecting in December and January if you're collecting data during during during the submission. That can be kind of rolled into the next.\
\
There will be a missing code most of the time that's 999 but there's different ones in some different structures so that's something you would want to check into dictionary. If a data field in a structure is marked as recommended as opposed to require can you just didn't collect it you just leave it blank if it's required. If it's not required and you didn't collected just leave the data blank if you didn's collect it then you should upload it.}